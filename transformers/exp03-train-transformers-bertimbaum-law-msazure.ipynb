{"cells":[{"cell_type":"markdown","source":["# EXPERIMENTO 03 - TREINAMENTO DO MODELO UTILIZANDO A API DE TRANSFORMERS DO HUGGINGFACE E O MODELO PRÉ-TREINADO => https://huggingface.co/alfaneo/bertimbaulaw-base-portuguese-cased\n","\n","Modelo ajustado com termos jurídicos com base no modelo pré-treinado: https://huggingface.co/neuralmind/bert-base-portuguese-cased\n","\n","**Ambiente Microsoft Azure - Treinamento da dissertação**"],"metadata":{"id":"RLv75vxhauAx"},"id":"RLv75vxhauAx"},{"cell_type":"markdown","source":["**INSTALAÇÃO DE DEPENDÊNCIAS**"],"metadata":{"nteract":{"transient":{"deleting":false}},"id":"c0e623bc-78be-47a4-8b5c-7bb65df3a24a"},"id":"c0e623bc-78be-47a4-8b5c-7bb65df3a24a"},{"cell_type":"code","source":["!pip install transformers datasets torch tqdm numpy pandas py7zr rouge_score"],"outputs":[],"execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T12:48:55.024550Z","iopub.status.busy":"2023-02-19T12:48:55.019893Z","iopub.status.idle":"2023-02-19T12:49:00.521171Z","shell.execute_reply":"2023-02-19T12:49:00.520449Z","shell.execute_reply.started":"2023-02-19T12:48:55.024506Z"},"gather":{"logged":1676834796922},"id":"3adf66f0-1464-4a4d-a33c-4eff00e30551"},"id":"3adf66f0-1464-4a4d-a33c-4eff00e30551"},{"cell_type":"markdown","source":["**IMPORTAÇÃO DAS  BIBLIOTECAS**"],"metadata":{"nteract":{"transient":{"deleting":false}},"id":"3fdc8f6d-2056-4aeb-bf19-eea1b5988f0b"},"id":"3fdc8f6d-2056-4aeb-bf19-eea1b5988f0b"},{"cell_type":"code","source":["from datasets import load_dataset\n","from transformers import AutoTokenizer\n","from torch.utils.data import DataLoader\n","from transformers import EncoderDecoderModel\n","from datasets import load_metric\n","import torch\n","from tqdm.notebook import tqdm\n","from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer"],"outputs":[],"execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T12:49:00.524164Z","iopub.status.busy":"2023-02-19T12:49:00.523865Z","iopub.status.idle":"2023-02-19T12:49:12.975591Z","shell.execute_reply":"2023-02-19T12:49:12.974818Z","shell.execute_reply.started":"2023-02-19T12:49:00.524129Z"},"gather":{"logged":1676978286651},"id":"856ee015-f9a6-4dc7-bd4f-4d7eb0fe4b39"},"id":"856ee015-f9a6-4dc7-bd4f-4d7eb0fe4b39"},{"cell_type":"markdown","source":["**MONTAGEM DO DATASET COM OS PARES DE SENTENÇAS**"],"metadata":{"nteract":{"transient":{"deleting":false}},"id":"d631c847-0caf-4f31-a948-ba668d7a4d98"},"id":"d631c847-0caf-4f31-a948-ba668d7a4d98"},{"cell_type":"code","source":["PATH = '/mnt/batch/tasks/shared/LS_root/mounts/clusters/computerexperimento03/code/Users/alexandre.alves.net'\n","\n","# Dataset/Downloads dos dados\n","dataset = load_dataset(PATH, data_files='amostra_sentences_stf_pt_13288.json', split='train', field=\"data\")\n","\n","ds = dataset.train_test_split(test_size=0.05)\n","train_data = ds['train'].shuffle(seed=42)\n","val_data = ds['test']"],"outputs":[{"output_type":"stream","name":"stdout","text":"Downloading and preparing dataset json/alexandre.alves.net to /home/azureuser/.cache/huggingface/datasets/json/alexandre.alves.net-84014f0e3307bd0c/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5...\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a489b9878db40959d006be82f8aaa10"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":"Using custom data configuration alexandre.alves.net-84014f0e3307bd0c\n"},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d29ed5098810438fa1cee4b6ced4286d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 tables [00:00, ? tables/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f63238a1e25b470a8ce711dd004feb2b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":"Dataset json downloaded and prepared to /home/azureuser/.cache/huggingface/datasets/json/alexandre.alves.net-84014f0e3307bd0c/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5. Subsequent calls will reuse this data.\n"}],"execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T12:49:12.977757Z","iopub.status.busy":"2023-02-19T12:49:12.976966Z","iopub.status.idle":"2023-02-19T12:49:13.095597Z","shell.execute_reply":"2023-02-19T12:49:13.094762Z","shell.execute_reply.started":"2023-02-19T12:49:12.977729Z"},"gather":{"logged":1676978527980},"colab":{"referenced_widgets":["1a489b9878db40959d006be82f8aaa10","d29ed5098810438fa1cee4b6ced4286d","f63238a1e25b470a8ce711dd004feb2b"]},"id":"baa9d306-cf6d-4188-bf30-83127afc9456","outputId":"92d21f94-0f94-41ee-d904-3dd4e6e06325"},"id":"baa9d306-cf6d-4188-bf30-83127afc9456"},{"cell_type":"markdown","source":["**DEFINIÇÃO DO MODELO**"],"metadata":{"nteract":{"transient":{"deleting":false}},"id":"ae58df3c-98f6-43ba-a855-fa4f43eaf519"},"id":"ae58df3c-98f6-43ba-a855-fa4f43eaf519"},{"cell_type":"code","source":["model_name = 'alfaneo/bertimbaulaw-base-portuguese-cased'\n","ds_col_in = 'original'\n","ds_col_out = 'simples'\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","assert tokenizer.is_fast\n","\n","encoder_max_length = 512\n","decoder_max_length = 512\n","\n","\n","def process_data_to_model_inputs(batch):\n","    # tokenize the inputs and labels\n","\n","    inputs = tokenizer(batch[ds_col_in], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n","    outputs = tokenizer(batch[ds_col_out], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n","\n","    batch[\"input_ids\"] = inputs.input_ids\n","    batch[\"attention_mask\"] = inputs.attention_mask\n","    batch[\"labels\"] = outputs.input_ids.copy()\n","\n","    # We have to make sure that the PAD token is ignored by the loss function\n","    batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n","\n","    return batch\n","\n","v_batch_size = 8\n","\n","train_data = train_data.map(\n","    process_data_to_model_inputs,\n","    batched=True,\n","    batch_size=v_batch_size,\n","    remove_columns=[ds_col_in, ds_col_out]\n",")\n","val_data = val_data.map(\n","    process_data_to_model_inputs,\n","    batched=True,\n","    batch_size=v_batch_size,\n","    remove_columns=[ds_col_in, ds_col_out]\n",")\n","\n","train_data.set_format(\n","    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",")\n","\n","val_data.set_format(\n","    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",")"],"outputs":[{"output_type":"stream","name":"stderr","text":"Parameter 'function'=<function process_data_to_model_inputs at 0x7fd48b9228b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1578 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d908fbb18d540889dea7307aed5613d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/84 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc937068384b4fa880d41cffd98d30a0"}},"metadata":{}}],"execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T12:49:13.097839Z","iopub.status.busy":"2023-02-19T12:49:13.097401Z","iopub.status.idle":"2023-02-19T12:49:16.725107Z","shell.execute_reply":"2023-02-19T12:49:16.724415Z","shell.execute_reply.started":"2023-02-19T12:49:13.097815Z"},"gather":{"logged":1676978550449},"colab":{"referenced_widgets":["8d908fbb18d540889dea7307aed5613d","cc937068384b4fa880d41cffd98d30a0"]},"id":"834ce007-7da9-47c6-b3b9-3db8e8090dff","outputId":"3c3023b1-b889-4a10-f7d0-d17589bd69b1"},"id":"834ce007-7da9-47c6-b3b9-3db8e8090dff"},{"cell_type":"markdown","source":["**TESTES INICIAIS**"],"metadata":{"nteract":{"transient":{"deleting":false}},"id":"22c11a59-53f9-4aac-9fb1-3866f45caad4"},"id":"22c11a59-53f9-4aac-9fb1-3866f45caad4"},{"cell_type":"code","source":["train_dataloader = DataLoader(train_data, shuffle=True, batch_size=v_batch_size)\n","val_dataloader = DataLoader(val_data, batch_size=v_batch_size)\n","\n","batch = next(iter(train_dataloader))\n","\n","for k,v in batch.items():\n","  print(k, v.shape)\n","print('---------------------------------------------------------')\n","print(tokenizer.decode(batch[\"input_ids\"][0].tolist()))\n","print('---------------------------------------------------------')\n","labels = batch[\"labels\"][0].tolist()\n","labels = [label for label in labels if label != -100 ]\n","tokenizer.decode(labels)"],"outputs":[{"output_type":"stream","name":"stdout","text":"input_ids torch.Size([8, 512])\nattention_mask torch.Size([8, 512])\nlabels torch.Size([8, 512])\n---------------------------------------------------------\n[CLS] O primeiro versa sobre mera questão de fato ; o segundo, ao revés, sobre questão de direito. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n---------------------------------------------------------\n"},{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"'[CLS] O primeiro é sobre mera questão de fato ; o segundo é sobre questão de direito. [SEP]'"},"metadata":{}}],"execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T12:49:16.726616Z","iopub.status.busy":"2023-02-19T12:49:16.725924Z","iopub.status.idle":"2023-02-19T12:49:16.810886Z","shell.execute_reply":"2023-02-19T12:49:16.810274Z","shell.execute_reply.started":"2023-02-19T12:49:16.726586Z"},"gather":{"logged":1676978557215},"id":"798c085e-8d34-41f5-b6b5-8a6758043bdb","outputId":"c3da6ef5-b67d-4af6-dca1-83815225151e"},"id":"798c085e-8d34-41f5-b6b5-8a6758043bdb"},{"cell_type":"markdown","source":["**MÉTRICAS**"],"metadata":{"nteract":{"transient":{"deleting":false}},"id":"03f18462-26c6-43ce-b6cd-3d5a14d9694f"},"id":"03f18462-26c6-43ce-b6cd-3d5a14d9694f"},{"cell_type":"code","source":["# Métricas\n","rouge = load_metric(\"rouge\")\n","\n","\n","def compute_rouge(pred_ids, label_ids):\n","    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","    label_ids[label_ids == -100] = tokenizer.pad_token_id\n","    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n","\n","    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n","\n","    return {\n","        \"rouge2_precision\": round(rouge_output.precision, 4),\n","        \"rouge2_recall\": round(rouge_output.recall, 4),\n","        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n","    }"],"outputs":[],"execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T12:49:16.812272Z","iopub.status.busy":"2023-02-19T12:49:16.811774Z","iopub.status.idle":"2023-02-19T12:49:17.357982Z","shell.execute_reply":"2023-02-19T12:49:17.357294Z","shell.execute_reply.started":"2023-02-19T12:49:16.812247Z"},"gather":{"logged":1676978567973},"id":"f2398798-9ce2-4794-8998-6e8a69a6db65"},"id":"f2398798-9ce2-4794-8998-6e8a69a6db65"},{"cell_type":"markdown","source":["**TREINAMENTO E DEFINIÇÃO DOS PARÂMETROS**"],"metadata":{"nteract":{"transient":{"deleting":false}},"id":"c3d95e3d-8bbe-4e39-835c-2d726ce9cb7b"},"id":"c3d95e3d-8bbe-4e39-835c-2d726ce9cb7b"},{"cell_type":"code","source":["# Treinamento\n","model = EncoderDecoderModel.from_encoder_decoder_pretrained(model_name, model_name)\n","model.config.decoder_start_token_id = tokenizer.cls_token_id\n","model.config.eos_token_id = tokenizer.sep_token_id\n","model.config.pad_token_id = tokenizer.pad_token_id\n","model.config.vocab_size = model.config.encoder.vocab_size\n","# settings for the generate() method\n","model.config.max_length = 120\n","model.config.min_length = 40\n","model.config.no_repeat_ngram_size = 3\n","model.config.early_stopping = True\n","model.config.length_penalty = 0.8\n","model.config.num_beams = 3\n","\n","training_arguments = Seq2SeqTrainingArguments(\n","    predict_with_generate=True,\n","    evaluation_strategy='steps',\n","    num_train_epochs=10,\n","    per_device_train_batch_size=v_batch_size,\n","    per_device_eval_batch_size=v_batch_size,\n","    fp16=torch.cuda.is_available(),\n","    output_dir=PATH + '/output',\n","    logging_steps=100,\n","    save_steps=3000,\n","    eval_steps=10000,\n","    warmup_steps=2000,\n","    gradient_accumulation_steps=1,\n","    save_total_limit=3\n",")\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    compute_metrics=compute_rouge,\n","    train_dataset=train_data,\n","    eval_dataset=val_data\n",")\n","\n","trainer.train()\n","trainer.save_model(PATH + '/model')"],"outputs":[{"output_type":"stream","name":"stderr","text":"Some weights of the model checkpoint at juridics/bertimbaulaw-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertModel were not initialized from the model checkpoint at juridics/bertimbaulaw-base-portuguese-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertLMHeadModel were not initialized from the model checkpoint at juridics/bertimbaulaw-base-portuguese-cased and are newly initialized: ['bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/anaconda/envs/azureml_py38/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n***** Running training *****\n  Num examples = 12623\n  Num Epochs = 10\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 15780\nTrainer is attempting to log a value of \"{'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'chunk_size_feed_forward': 0, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'architectures': ['BertForMaskedLM'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': 0, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'juridics/bertimbaulaw-base-portuguese-cased', 'transformers_version': '4.16.0', 'directionality': 'bidi', 'output_past': True, 'pooler_fc_size': 768, 'pooler_num_attention_heads': 12, 'pooler_num_fc_layers': 3, 'pooler_size_per_head': 128, 'pooler_type': 'first_token_transform', 'vocab_size': 29794, 'hidden_size': 768, 'num_hidden_layers': 12, 'num_attention_heads': 12, 'hidden_act': 'gelu', 'intermediate_size': 3072, 'hidden_dropout_prob': 0.1, 'attention_probs_dropout_prob': 0.1, 'max_position_embeddings': 512, 'type_vocab_size': 2, 'initializer_range': 0.02, 'layer_norm_eps': 1e-12, 'position_embedding_type': 'absolute', 'use_cache': True, 'classifier_dropout': None, 'model_type': 'bert'}\" for key \"encoder\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute.\nTrainer is attempting to log a value of \"{'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'is_encoder_decoder': False, 'is_decoder': True, 'cross_attention_hidden_size': None, 'add_cross_attention': True, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'chunk_size_feed_forward': 0, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'architectures': ['BertForMaskedLM'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': 0, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'juridics/bertimbaulaw-base-portuguese-cased', 'transformers_version': '4.16.0', 'directionality': 'bidi', 'output_past': True, 'pooler_fc_size': 768, 'pooler_num_attention_heads': 12, 'pooler_num_fc_layers': 3, 'pooler_size_per_head': 128, 'pooler_type': 'first_token_transform', 'vocab_size': 29794, 'hidden_size': 768, 'num_hidden_layers': 12, 'num_attention_heads': 12, 'hidden_act': 'gelu', 'intermediate_size': 3072, 'hidden_dropout_prob': 0.1, 'attention_probs_dropout_prob': 0.1, 'max_position_embeddings': 512, 'type_vocab_size': 2, 'initializer_range': 0.02, 'layer_norm_eps': 1e-12, 'position_embedding_type': 'absolute', 'use_cache': True, 'classifier_dropout': None, 'model_type': 'bert'}\" for key \"decoder\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute.\n/anaconda/envs/azureml_py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"},{"output_type":"stream","name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1552' max='15780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1552/15780 15:29:52 < 142:15:42, 0.03 it/s, Epoch 0.98/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":"Attempted to log scalar metric loss:\n10.3457\nAttempted to log scalar metric learning_rate:\n2.5e-06\nAttempted to log scalar metric epoch:\n0.06\nAttempted to log scalar metric loss:\n5.804\nAttempted to log scalar metric learning_rate:\n5e-06\nAttempted to log scalar metric epoch:\n0.13\nAttempted to log scalar metric loss:\n4.0619\nAttempted to log scalar metric learning_rate:\n7.5e-06\nAttempted to log scalar metric epoch:\n0.19\nAttempted to log scalar metric loss:\n3.2184\nAttempted to log scalar metric learning_rate:\n1e-05\nAttempted to log scalar metric epoch:\n0.25\nAttempted to log scalar metric loss:\n2.7232\nAttempted to log scalar metric learning_rate:\n1.25e-05\nAttempted to log scalar metric epoch:\n0.32\nAttempted to log scalar metric loss:\n2.446\nAttempted to log scalar metric learning_rate:\n1.5e-05\nAttempted to log scalar metric epoch:\n0.38\nAttempted to log scalar metric loss:\n2.2503\nAttempted to log scalar metric learning_rate:\n1.75e-05\nAttempted to log scalar metric epoch:\n0.44\nAttempted to log scalar metric loss:\n2.1526\nAttempted to log scalar metric learning_rate:\n2e-05\nAttempted to log scalar metric epoch:\n0.51\nAttempted to log scalar metric loss:\n2.0335\nAttempted to log scalar metric learning_rate:\n2.25e-05\nAttempted to log scalar metric epoch:\n0.57\nAttempted to log scalar metric loss:\n1.8984\nAttempted to log scalar metric learning_rate:\n2.5e-05\nAttempted to log scalar metric epoch:\n0.63\nAttempted to log scalar metric loss:\n1.8487\nAttempted to log scalar metric learning_rate:\n2.7500000000000004e-05\nAttempted to log scalar metric epoch:\n0.7\nAttempted to log scalar metric loss:\n1.7968\nAttempted to log scalar metric learning_rate:\n3e-05\nAttempted to log scalar metric epoch:\n0.76\nAttempted to log scalar metric loss:\n1.7177\nAttempted to log scalar metric learning_rate:\n3.2500000000000004e-05\nAttempted to log scalar metric epoch:\n0.82\nAttempted to log scalar metric loss:\n1.6331\nAttempted to log scalar metric learning_rate:\n3.5e-05\nAttempted to log scalar metric epoch:\n0.89\nAttempted to log scalar metric loss:\n1.6734\nAttempted to log scalar metric learning_rate:\n3.7500000000000003e-05\nAttempted to log scalar metric epoch:\n0.95\n"},{"output_type":"stream","name":"stderr","text":"Bad pipe message: %s [b'\\x14\\xaf\\xb5\\x90;<\\x1f\\n\\xd2\\xb1\\x9c-\\x14%\\x05\\xd9.T \\x16\\xd5l\\xad){U;\\xda\"_\\xac\\xbeq4\\xe0\\xfeEiO\\xfaaK\\xf9\\x87:\\xeb\\xd0O\\xe4\\xbb\\xfb\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x00+\\x00\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 \\x7f\\xe7\\xce\\xbcqU\\x9dF=%\\x06E]\\xde\\xc8\\xa2\"<']\nBad pipe message: %s [b\"\\xa9\\x0bBF\\xa5\\xc7o\\x1c\\x16\\x86\\xccw\\xa0_\\xd2\\x17\\xe1F\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\"]\nBad pipe message: %s [b'\\x05\\x03\\x06', b'\\x07\\x08']\nBad pipe message: %s [b'\\t\\x08\\n\\x08\\x0b\\x08\\x04']\nBad pipe message: %s [b'\\x08\\x06\\x04\\x01\\x05\\x01\\x06', b'', b'\\x03\\x03']\nBad pipe message: %s [b'']\nBad pipe message: %s [b'', b'\\x02']\nBad pipe message: %s [b'\\x05\\x02\\x06']\nBad pipe message: %s [b'\\xe8\\x96']\nBad pipe message: %s [b'\\xbd\\xbfQ{v\\x1fl\\xd4\\x83\\xf7O\\xf8\\rx\\xaf\\x81\\xffS\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01']\nBad pipe message: %s [b'\\n']\nBad pipe message: %s [b'\\x95\\xdfbg6\\xf7\\xc1\\x94\\xe9\\xf0\\xc15\\xf6\\xe7\\x14(#\\xab\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t']\nBad pipe message: %s [b'', b\"]4\\x18\\x89\\xb3\\x92\\xaeI\\x1cW\\x80)\\x8f\\x81\\x8fE\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\"]\nBad pipe message: %s [b'&\\xb2\\x89\\xbei\\\\\\xd5]\\x9b\\x03\\xedTu`i\\xcf\\\\\\xd2\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00']\nBad pipe message: %s [b\":\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00\"]\nBad pipe message: %s [b'\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00']\nBad pipe message: %s [b'\\x10\\xc0']\nBad pipe message: %s [b'\\x15\\xc0\\x0b\\xc0\\x01']\nBad pipe message: %s [b'\\xe8\\xf6N]\\xff]\\xcdqA\\xe9\\x01\\xcfM+\\xd3{!( \\xc2K\\x13\\xcb6<\\x80\\xf7\\x93\\xd7U.\\xb1=\\x1ct\\xa7\\xecA\\xf0s\\xef\\xc5\\r\\xc7\\x87\\n\\xf7I\\x81YE\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x00+\\x00\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 ~\\xce\\x1d0\\xed\\x94\\xed\\x1e|\\xc2\\xd1\\x8f\\x0b\\x9d']\nBad pipe message: %s [b'\\xacq\\xe5\\xd3\\x92\\xa1\\x17\\xf0\\xb0I\\xb7\\xfd\\x7f\\xdf\\x1c+\\x84\\xf2 \\x844c\\xf0\\x88E \\xeaL\\x02\\xfa\\xf5\\xf2\\x9a\\x8a\\xac\\xa5w\\x81\\xfb2D\\xb26x\\xca\\x02\\x06\\xeeY\\xb7\\xc3\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0']\nBad pipe message: %s [b\"f\\xf0\\xb9\\x12\\xd7L\\x10)\\x8a\\xa9N0c*\\xf3\\xbb;\\xab\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x00\", b'8\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00']\nBad pipe message: %s [b'\\x00\\t127.0.0.1']\nBad pipe message: %s [b'.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04']\nBad pipe message: %s [b'\\x03\\x06', b'\\x07\\x08']\nBad pipe message: %s [b'\\t\\x08\\n\\x08\\x0b\\x08\\x04']\nBad pipe message: %s [b'\\x08\\x06\\x04\\x01\\x05\\x01\\x06', b'']\nBad pipe message: %s [b'\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 \\xf4\\x955\\xe9\\x985\\x84W\\xa0\\x03\\xaeB\\xa1\\xf4\\xad\\xc4\\x1f7Qk6\\x8c']\nBad pipe message: %s [b\"d'\\\\S\\xcd\\x02\\xcf\\xbaf1d\\x87\\x96M\\xf8\\xdd\\xe4\\x1b\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\x0f\\x00\\x01\\x01\\x15\\x03\\x02\"]\nBad pipe message: %s [b'\\x87\\x93\\xc7\\xef\\xb2V\\x19.D\\xa5\\xa9\\xe7\\xdc\\xfeHC\\xc3v\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0']\nBad pipe message: %s [b'.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00']\nBad pipe message: %s "}],"execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T12:49:17.361328Z","iopub.status.busy":"2023-02-19T12:49:17.360920Z"},"gather":{"logged":1676924705561},"id":"59530a0e-265c-4cab-bee6-07f256c0d69a","outputId":"64a48f4b-e991-4729-95b4-e9da02666490"},"id":"59530a0e-265c-4cab-bee6-07f256c0d69a"}],"metadata":{"kernelspec":{"name":"python38-azureml","language":"python","display_name":"Python 3.8 - AzureML"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"microsoft":{"ms_spell_check":{"ms_spell_check_language":"en"},"host":{"AzureML":{"notebookHasBeenCompleted":true}}},"kernel_info":{"name":"python38-azureml"},"nteract":{"version":"nteract-front-end@1.0.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}